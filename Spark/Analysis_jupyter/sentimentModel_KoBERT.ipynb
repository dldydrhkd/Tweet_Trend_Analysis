{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentimentModel_KoBERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMC3V7bpIA9HBgHCS0KklDe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3.0.2\n","!pip install torch"],"metadata":{"id":"cxD9Nm7YrMnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install qiskit"],"metadata":{"id":"IzIy4rMlwnYO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"hakbDH0Ay_6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"],"metadata":{"id":"F39ZQCJp0IiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model"],"metadata":{"id":"bd3q0T2_0eNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"9nKiiQOM0lO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pytorch 에서 gpu 사용 선택 (0번 아이디를 가진 gpu를 사용)\n","device = torch.device(\"cuda:0\") "],"metadata":{"id":"6mQhWH1v0pDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사전 훈련된 kobert model 불러옴\n","bertmodel, vocab = get_pytorch_kobert_model()"],"metadata":{"id":"oef13WxP0sxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3OoUbB5Z0ylY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브에서 학습시킬 데이터 셋 가져오기\n","# /content/drive/MyDrive/ == 기본 경로\n","import pandas as pd\n","sentiment_data = pd.read_excel('/content/drive/MyDrive/AIHUB_dataset_customizing.xlsx')"],"metadata":{"id":"BTk9pTwF1vdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 셋 크기 확인\n","len(sentiment_data)"],"metadata":{"id":"4SZHM4ok1-td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 랜덤으로 샘플데이터 10개 출력\n","sentiment_data.sample(n=10)"],"metadata":{"id":"HDPCfHKV2Gov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 셋 같은 경우, 0~2 까지 3개의 분류로 구성되어 있음 \n","# 해당 데이터 셋의 공포, 놀람 등 항목을 사용하기 편하도록 숫자로 변경\n","sentiment_data.loc[(sentiment_data['Emotion'] == \"긍정\"), 'Emotion'] = 0  #긍정 => 0\n","sentiment_data.loc[(sentiment_data['Emotion'] == \"부정\"), 'Emotion'] = 1  #부정 => 1\n","sentiment_data.loc[(sentiment_data['Emotion'] == \"중립\"), 'Emotion'] = 2  #중립 => 2"],"metadata":{"id":"TMgujd0C2JDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터 셋 구성\n","# 가져온 데이터 셋 파일에서 sentence와 emotion 항목을 q와 label로 각각 구성하여 학습을 위한 데이터 셋으로 구성 \n","data_list = []\n","for q, label in zip(sentiment_data['Sentence'], sentiment_data['Emotion'])  :\n","    data = []\n","    data.append(q)\n","    data.append(str(label))\n","\n","    data_list.append(data)"],"metadata":{"id":"rScq2glE2Vdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 셋의 형태 확인\n","print(len(data_list))\n","print(data_list[0])\n","print(data_list[6000])\n","print(data_list[12000])\n","print(data_list[18000])\n","print(data_list[24000])\n","print(data_list[30000])\n","print(data_list[-1])"],"metadata":{"id":"zydjA90G2X3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sklearn.model_selection 패키지를 통하여 학습 데이터와 테스트 데이터로 분할\n","from sklearn.model_selection import train_test_split\n","\n","# 75%는 학습 데이터셋, 25%는 테스트 데이터셋으로 구성\n","dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"],"metadata":{"id":"cQZ2LbDY2aNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터셋과 테스트 데이터셋의 분할 확인 및 형태 확인\n","print(len(dataset_train))\n","print(len(dataset_test))\n","print(dataset_train[0])\n","print(dataset_test[0])"],"metadata":{"id":"hdKfbAsD2gW4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습에 사용할 데이터 셋 클래스 선언\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        # transform하여 토큰화 하는 과정에서 \n","        # transform(i[인덱스]) == transform(\"아아아아 아아아 아아\") 같은 경우 토큰화가 정상적으로 진행 안되는 오류 \n","        # transform([i[인덱스]]) == transform([\"아아아아 아아아 아아\"]) 형식으로 transform 진행\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    # 전체 데이터 셋에서 해당하는 인덱스의 sentences와 labels만 뽑는 함수\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    # 데이터 셋의 사이즈 return\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"H0EvcQFm2jfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 파라미터 설정\n","# 토큰의 최대 길이라고 생각\n","max_len = 64\n","# 몇 개의 샘플들을 예측해보고 가중치를 업데이트 할 지 설정\n","# 아래와 같이 배치 사이즈가 64인 경우 데이터 64개 마다 예측한 것을 실제 값과 비교한다\n","batch_size = 64\n","warmup_ratio = 0.1\n","# epoch 횟수는 모델이 전체 데이터셋을 훈련시킬 횟수를 의미한다.\n","num_epochs = 10\n","max_grad_norm = 1\n","log_interval = 200\n","# learning_rate 값이 너무 크면 원하는 값에 도달하기 힘들고, 너무 작으면 학습기간이 오래 걸린다.\n","learning_rate =  5e-5"],"metadata":{"id":"-vekeOkq2m-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# kobert에서 vocab을 통해서 토큰화 진행\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"metadata":{"id":"k079DQZr2pdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터셋과 테스트 데이터셋 토큰화 진행\n","data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"],"metadata":{"id":"tmB3uPwb2rME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰화 확인 (마지막에 호출된 데이터셋이 출력됨)\n","data_train[0]\n","# data_test[0]"],"metadata":{"id":"ah99TesG2vu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DataLoader를 통해서 전체 데이터셋이 batch_size로 분할\n","# num_workers의 경우 높을수록 load 속도 상승\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"],"metadata":{"id":"p0yDnkzg2zaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 분류에 사용할 모델 클래스 선언\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 # num_classes는 카테고리의 개수를 의미한다. (현재 데이터셋의 경우 3개의 분류로 데이터셋이 구성되어 있음)\n","                 num_classes=3,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"FL8N6GeZ21-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to(Device)는 위의 torch.device(\"cuda:0\")를 의미하며 \n","# GPU에서 학습된 모델을 GPU로 불러올 때 사용한다.\n","# 또한 GPU로 학습된 모델에 데이터를 제공할 때도 to(Device)를 붙여줘야 함.\n","# bert모델 불러오기\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"],"metadata":{"id":"0aR1PBUk27C1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer와 schedule 설정\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"metadata":{"id":"tgjxendf28sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer = 전체 데이터 셋의 실제 결과와 모델이 예측한 값 간의 차이가 효율적으로 좁혀질 수 있도록 최적화해주는 역할\n","# transform에서 제공하는 AdamW optimizer 사용\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","# 예측값과 실제값의 오차인 손실함수는 pytorch에서 제공하는 다중분류를 위한 대표적인 손실함수인 torch.nn.CrossEntropyLoss 사용\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"ITUoUHbN3AGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 총 학습 수는 학습데이터의 크기 * 에폭수\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"metadata":{"id":"uGzVJkuB3Bhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 시간이 지남에 따라 학습률을 조금씩 감소시키는 scheduler\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"metadata":{"id":"rHNC3-W83DHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정확도 측정 함수\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"metadata":{"id":"g7iZ08CR3FIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습 \n","# 에폭수 만큼 반복\n","for e in range(num_epochs):\n","    # 정확도 초기화\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    # 학습 모드\n","    model.train()\n","\n","    # 배치만큼 학습 데이터셋 가져옴\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        # 그래디언트 = 순간변화율\n","        # 그래디언트 초기화\n","        optimizer.zero_grad()\n","\n","        # 배치에서 데이터 추출\n","        # 모델에 제공하는 데이터이기때문에 to(device)\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        # output\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        # 손실 계산 (실제값과 예측값 간의 오차)\n","        loss = loss_fn(out, label)\n","\n","        # 그래디언트에 값 더하기\n","        loss.backward()\n","\n","        # 학습의 안정화를 위해 그래디언트 클리핑(자르기) => 기울기가 너무 커지는 것을 방지\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        # 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 학습률 감소 스케줄러 \n","        scheduler.step()\n","\n","        # 정확도\n","        train_acc += calc_accuracy(out, label)\n","\n","        # batch 200 간격으로 학습상황 출력\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    # 평가 모드\n","    model.eval()\n","\n","    # 테스트 데이터셋을 통한 평가\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"metadata":{"id":"_jkz0cVu3IQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UKVN9Gex3M-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 기본 경로 = '/content/drive/MyDrive'\n","import os\n","# 해당 디렉토리로 이동\n","os.chdir('/content/drive/MyDrive/models')\n","# 현재 경로\n","os.getcwd()"],"metadata":{"id":"RskT5i2EPnhS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/models/'\n","# 지정한 path에 전체 모델 저장\n","torch.save(model, path + 'sentiment_model.pt') \n","\n","# dict 저장\n","# torch.save(model.state_dict(), '7emotions_model_state_dict.pt')\n","\n","# 필요한 값 지정해서 저장 가능\n","# torch.save({\n","#     'model': model.state_dict(),\n","#     'optimizer': optimizer.state_dict()\n","# }, '7emotions_all.tar')  "],"metadata":{"id":"tsySPPOsPtjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fpA94F7L1-VV"},"execution_count":null,"outputs":[]}]}