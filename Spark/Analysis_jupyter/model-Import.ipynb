{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model-Import.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVhGRLi5hNGwBMTejaPwQB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3.0.2\n","!pip install torch"],"metadata":{"id":"0Lv784XpUAr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install qiskit"],"metadata":{"id":"v2Jmwc2imT30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"pJcgiAXhoJD6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"],"metadata":{"id":"8cFFEOjpSyEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model"],"metadata":{"id":"dO9AXb99n7GF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"JxOr3HvhoRSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pytorch 에서 cpu 사용 선택\n","# cpu 환경에서 모델 구동시\n","device = torch.device(\"cpu\")"],"metadata":{"id":"7vi9AuzYXy8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사전 훈련된 kobert model 불러옴\n","bertmodel, vocab = get_pytorch_kobert_model()"],"metadata":{"id":"9b72yHBxn20t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습에 사용할 데이터 셋 클래스 선언\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"Xzv6CQifqtp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 파라미터 설정\n","# 토큰의 최대 길이라고 생각\n","max_len = 64\n","# 몇 개의 샘플들을 예측해보고 가중치를 업데이트 할 지 설정\n","# 아래와 같이 배치 사이즈가 64인 경우 데이터 64개 마다 예측한 것을 실제 값과 비교한다\n","batch_size = 64\n","warmup_ratio = 0.1\n","# epoch 횟수는 모델이 전체 데이터셋을 훈련시킬 횟수를 의미한다.\n","num_epochs = 10\n","max_grad_norm = 1\n","log_interval = 200\n","# learning_rate 값이 너무 크면 원하는 값에 도달하기 힘들고, 너무 작으면 학습기간이 오래 걸린다.\n","learning_rate =  5e-5"],"metadata":{"id":"nn-u3SJQqwpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 분류에 사용할 모델 클래스 선언\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 # num_classes는 카테고리의 개수를 의미한다. (현재 데이터셋의 경우 3개의 분류로 데이터셋이 구성되어 있음)\n","                 num_classes=3,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"hXoMf6WkWDll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to(Device)는 위의 torch.device(\"cuda:0\")를 의미하며 \n","# GPU에서 학습된 모델을 GPU로 불러올 때 사용한다.\n","# 또한 GPU로 학습된 모델에 데이터를 제공할 때도 to(Device)를 붙여줘야 함.\n","# bert모델 불러오기\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"],"metadata":{"id":"-sGS74oInt3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer와 schedule 설정\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"metadata":{"id":"kduIcdYcWyWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer = 전체 데이터 셋의 실제 결과와 모델이 예측한 값 간의 차이가 효율적으로 좁혀질 수 있도록 최적화해주는 역할\n","# transform에서 제공하는 AdamW optimizer 사용\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","# 예측값과 실제값의 오차인 손실함수는 pytorch에서 제공하는 다중분류를 위한 대표적인 손실함수인 torch.nn.CrossEntropyLoss 사용\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"iGU-MBqtpLvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"js8qxsluo5cJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# 위치 변경\n","os.chdir('/content/drive/MyDrive/models/')"],"metadata":{"id":"nFL1XXh2o9D_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 모델 불러오기(cpu 환경으로 사용할 때)\n","sentimentModel = torch.load('./sentiment_model.pt',map_location='cpu')\n","\n","# dict 불러오기\n","# model1.load_state_dict(torch.load('7emotions_model_state_dict.pt'))  \n","\n","# 지정한 값 불러오기\n","# checkpoint = torch.load('7emotions_all.tar')   \n","# model1.load_state_dict(checkpoint['model'])\n","# optimizer.load_state_dict(checkpoint['optimizer'])"],"metadata":{"id":"mGcvmdF4S3EJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 앞에 클래스를 선언해줘야 모델 실행 가능\n","\n","# kobert에서 vocab을 통해서 토큰화 진행\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","# 테스트 함수 \n","def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n","    \n","    # 평가 모드\n","    sentimentModel.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = sentimentModel(token_ids, valid_length, segment_ids)\n","\n","        test_eval=[]\n","\n","        # 학습위해 변환한 분류값 초기 분류값으로 변환\n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0:\n","                test_eval.append(\"긍정\")\n","            elif np.argmax(logits) == 1:\n","                test_eval.append(\"부정\")\n","            elif np.argmax(logits) == 2:\n","                test_eval.append(\"중립\")\n","\n","        print(test_eval[0])"],"metadata":{"id":"5D-fDl3KnZdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = input(\"검색문장: \")\n","predict(sentence)"],"metadata":{"id":"T_BD3Dvdq5p3"},"execution_count":null,"outputs":[]}]}